# -*- coding: utf-8 -*-
"""data_preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y8hf0qWHHmzSTu7fH0Z7bNbjsQ7aDIec
"""

import re
import html
import pandas as pd

from google.colab import files
uploaded = files.upload()

file_name = list(uploaded.keys())[0]
df = pd.read_csv(file_name)

print("Shape:", df.shape)
df.head()

df.columns

# If dataset has 'category' (huggingface), create 'Category'
if "Category" not in df.columns and "category" in df.columns:
    df["Category"] = df["category"]

# Now check unique categories
df["Category"].unique()

df = df.copy()

if "Subject" in df.columns and "Email_Body" in df.columns:
    df["raw_email"] = (df["Subject"].fillna("") + " " + df["Email_Body"].fillna("")).str.strip()

elif "subject" in df.columns and "body" in df.columns:
    df["raw_email"] = (df["subject"].fillna("") + " " + df["body"].fillna("")).str.strip()

elif "text" in df.columns:
    df["raw_email"] = df["text"].fillna("")

else:
    raise ValueError("Dataset must contain ('Subject' & 'Email_Body') OR ('subject' & 'body') OR ('text').")

df[["raw_email"]].head()

from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS
STOPWORDS = set(ENGLISH_STOP_WORDS)

len(STOPWORDS), list(sorted(list(STOPWORDS)))[:20]

import nltk
nltk.download("wordnet")
nltk.download("omw-1.4")

from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()

THREAD_PATTERNS = [
    r"-----original message-----",
    r"^from:.*$",
    r"^sent:.*$",
    r"^to:.*$",
    r"^subject:.*$",
    r"on\s.+?wrote:",
    r"begin forwarded message",
    r"forwarded message",
]

DISCLAIMER_PATTERNS = [
    r"confidentiality notice.*",
    r"this email.*confidential.*",
    r"intended only for.*",
    r"please consider the environment.*",
    r"do not print.*",
]

def remove_email_threads(text: str) -> str:
    lower = text.lower()
    cut_idx = None
    for pat in THREAD_PATTERNS:
        m = re.search(pat, lower, flags=re.IGNORECASE | re.MULTILINE)
        if m:
            cut_idx = m.start() if cut_idx is None else min(cut_idx, m.start())
    return text if cut_idx is None else text[:cut_idx]

def remove_signatures(text: str) -> str:
    lines = text.splitlines()
    cleaned_lines = []
    for line in lines:
        cleaned_lines.append(line)
        if re.match(r"^\s*(thanks|regards|best|sincerely|kind regards|warm regards)\b", line.strip().lower()):
            break
    return "\n".join(cleaned_lines)

def remove_disclaimers(text: str) -> str:
    out = text
    for pat in DISCLAIMER_PATTERNS:
        out = re.sub(pat, " ", out, flags=re.IGNORECASE | re.DOTALL)
    return out

def normalize_spacing(text: str) -> str:
    text = re.sub(r"\s+", " ", text)
    return text.strip()

def remove_html_tags(text: str) -> str:
    text = html.unescape(text)
    text = re.sub(r"<[^>]+>", " ", text)
    return text

def to_lower(text: str) -> str:
    return text.lower()

def remove_special_chars_numbers(text: str) -> str:
    # keep only a-z and spaces
    text = re.sub(r"[^a-z\s]", " ", text)
    text = re.sub(r"\s+", " ", text)
    return text.strip()

def tokenize(text: str):
    return [t for t in text.split() if t]

def remove_stopwords(tokens):
    return [t for t in tokens if t not in STOPWORDS and len(t) > 2]

def lemmatize_tokens(tokens):
    return [lemmatizer.lemmatize(t) for t in tokens]

def limit_max_length(tokens, max_tokens=200):
    return tokens[:max_tokens]

def clean_email_text(raw_text: str, max_tokens: int = 200) -> str:
    if raw_text is None:
        return ""
    text = str(raw_text)

    # Handle Long Emails
    text = remove_email_threads(text)
    text = remove_signatures(text)
    text = remove_disclaimers(text)
    text = normalize_spacing(text)

    # Remove HTML Tags
    text = remove_html_tags(text)

    # Lowercase
    text = to_lower(text)

    # Remove Special Characters & Numbers
    text = remove_special_chars_numbers(text)

    # Tokenization
    tokens = tokenize(text)

    # Stopword Removal
    tokens = remove_stopwords(tokens)

    # Lemmatization
    tokens = lemmatize_tokens(tokens)

    # Optional Stopword removal again
    tokens = remove_stopwords(tokens)

    # Limit length
    tokens = limit_max_length(tokens, max_tokens=max_tokens)

    return " ".join(tokens)

if "Subject" in df.columns and "Email_Body" in df.columns:
    df["raw_email"] = (df["Subject"].fillna("") + " " + df["Email_Body"].fillna("")).str.strip()

elif "subject" in df.columns and "body" in df.columns:
    df["raw_email"] = (df["subject"].fillna("") + " " + df["body"].fillna("")).str.strip()

elif "text" in df.columns:
    df["raw_email"] = df["text"].fillna("")

else:
    raise ValueError("Need ('Subject' & 'Email_Body') OR ('subject' & 'body') OR ('text').")

df["clean_text"] = df["raw_email"].apply(lambda x: clean_email_text(x, max_tokens=200))
df[["raw_email", "clean_text"]].head(10)

before = df.shape[0]

# remove empty clean_text rows
df = df[df["clean_text"].str.strip().astype(bool)].copy()

# remove duplicates
df = df.drop_duplicates(subset=["clean_text"]).reset_index(drop=True)

after = df.shape[0]
print("Before:", before, "After:", after, "Removed duplicates:", before - after)

HIGH_PATTERNS = [
    r"\burgent\b", r"\basap\b", r"\bimmediately\b", r"\bcritical\b",
    r"\bnot working\b", r"\bunable\b", r"\bfailure\b", r"\bfailed\b",
    r"\brefund\b", r"\bchargeback\b", r"\bdeadline\b"
]

MED_PATTERNS = [
    r"\bimportant\b", r"\bsoon\b", r"\bpriority\b", r"\bplease respond\b",
    r"\bdelay\b", r"\bfollow up\b", r"\bresponse\b"
]

def assign_urgency(text: str) -> str:
    t = str(text).lower()

    for pat in HIGH_PATTERNS:
        if re.search(pat, t):
            return "High"

    for pat in MED_PATTERNS:
        if re.search(pat, t):
            return "Medium"

    return "Low"

df["Urgency"] = df["clean_text"].apply(assign_urgency)
df["Urgency"].value_counts()

required_cols = ["clean_text", "Category", "Urgency"]

# keep only required columns
df_required = df[required_cols].copy()

print("Selected columns:", df_required.columns.tolist())
print("Final shape:", df_required.shape)

df_required.head()

# Count number of emails in each category
category_counts = df_required["Category"].value_counts()

print("Email count per Category:")
print(category_counts)

# Optional: show percentage also
category_percent = df_required["Category"].value_counts(normalize=True) * 100

print("\nCategory percentage (%):")
print(category_percent.round(2))

df_required.to_csv("clean_email.csv", index=False)
print("Saved: clean_email.csv")

from google.colab import files
files.download("clean_email.csv")