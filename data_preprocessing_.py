# -*- coding: utf-8 -*-
"""data_preprocessing..ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CbinnJr4Xj1sfvWHijVB8yLsCHC8uRER
"""

!pip -q install pandas numpy nltk beautifulsoup4 lxml

!pip -q install -U nltk

import pandas as pd
import numpy as np
import re
import nltk
from bs4 import BeautifulSoup

from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

import matplotlib.pyplot as plt

import nltk

nltk.download("punkt")
nltk.download("punkt_tab")   # IMPORTANT (newer nltk versions)
nltk.download("stopwords")
nltk.download("wordnet")
nltk.download("omw-1.4")     # optional but helpful for lemmatization

from google.colab import files

uploaded = files.upload()

# Get uploaded filename
file_name = list(uploaded.keys())[0]
print("Uploaded file:", file_name)

df = pd.read_csv(file_name)
print("Shape:", df.shape)
df.head()

df.columns = [c.strip().lower() for c in df.columns]

def find_col(possible_names):
    for name in possible_names:
        if name in df.columns:
            return name
    return None

subject_col = find_col(["subject", "email_subject", "title"])
body_col    = find_col(["body", "message", "email_body", "text", "content"])
category_col= find_col(["category", "label", "class"])
urgency_col = find_col(["urgency", "priority", "severity"])

print("Detected columns:")
print("subject_col:", subject_col)
print("body_col:", body_col)
print("category_col:", category_col)
print("urgency_col:", urgency_col)

# Fill missing with empty
if subject_col is not None:
    df[subject_col] = df[subject_col].fillna("")
else:
    df["subject_tmp"] = ""
    subject_col = "subject_tmp"

if body_col is not None:
    df[body_col] = df[body_col].fillna("")
else:
    raise ValueError("Body/Text column not found. Please ensure your CSV has body/text column.")

df["raw_email"] = (df[subject_col].astype(str) + " " + df[body_col].astype(str)).str.strip()

df[["raw_email"]].head()

df = df[df["raw_email"].str.strip().astype(bool)]
df = df.drop_duplicates(subset=["raw_email"]).reset_index(drop=True)

print("After removing empty + duplicates:", df.shape)

print("Missing values:\n", df.isnull().sum())

if category_col:
    print("\nCategory Distribution:\n", df[category_col].value_counts())

if urgency_col:
    print("\nUrgency Distribution:\n", df[urgency_col].value_counts())

df["email_length"] = df["raw_email"].astype(str).apply(len)

print(df["email_length"].describe())

plt.hist(df["email_length"], bins=30)
plt.title("Email Length Distribution")
plt.xlabel("Length")
plt.ylabel("Count")
plt.show()

stop_words = set(stopwords.words("english"))
lemmatizer = WordNetLemmatizer()

SIGNATURE_PATTERNS = [
    r"thanks[\s,!\-]*.*",
    r"regards[\s,!\-]*.*",
    r"best regards[\s,!\-]*.*",
    r"sincerely[\s,!\-]*.*",
    r"warm regards[\s,!\-]*.*",
    r"yours truly[\s,!\-]*.*",
]

def remove_html(text: str) -> str:
    return BeautifulSoup(text, "html.parser").get_text(" ")

def remove_signature(text: str) -> str:
    t = text
    # Common signature separator
    t = re.split(r"\n--\s*\n", t)[0]
    # Also cut off after common words like Thanks, Regards (heuristic)
    for pat in SIGNATURE_PATTERNS:
        t = re.sub(pat, "", t, flags=re.IGNORECASE | re.DOTALL)
    return t

def normalize_text(text: str) -> str:
    # lower
    text = text.lower()

    # remove urls
    text = re.sub(r"http\S+|www\.\S+", " ", text)

    # remove emails
    text = re.sub(r"\S+@\S+", " ", text)

    # remove phone-like numbers
    text = re.sub(r"\+?\d[\d\-\s]{7,}\d", " ", text)

    # remove non letters
    text = re.sub(r"[^a-z\s]", " ", text)

    # remove extra spaces
    text = re.sub(r"\s+", " ", text).strip()

    return text

def preprocess_email(text: str) -> str:
    if not isinstance(text, str):
        text = str(text)

    text = remove_html(text)
    text = remove_signature(text)
    text = normalize_text(text)

    tokens = word_tokenize(text)
    tokens = [w for w in tokens if w not in stop_words and len(w) > 2]
    tokens = [lemmatizer.lemmatize(w) for w in tokens]

    return " ".join(tokens)

df["clean_email"] = df["raw_email"].apply(preprocess_email)

# Remove rows that became empty after cleaning
df = df[df["clean_email"].str.strip().astype(bool)].reset_index(drop=True)

df[["raw_email", "clean_email"]].head(10)

sample = df.sample(5, random_state=42)[["raw_email", "clean_email"]]
sample

# Keep only useful columns
keep_cols = ["raw_email", "clean_email"]

if category_col:
    keep_cols.append(category_col)

if urgency_col:
    keep_cols.append(urgency_col)

clean_df = df[keep_cols].copy()
clean_df.to_csv("cleaned_emails.csv", index=False)

print("Saved:", "cleaned_emails.csv")
clean_df.head()

from google.colab import files
files.download("cleaned_emails.csv")